#!/usr/bin/env python3
"""
ü§ñ Evolution API Constructor/Consultor
Agente especializado em constru√ß√£o e consulta de APIs Evolution com busca h√≠brida IA + textual
"""

import json
import os
import re
import unicodedata
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import anthropic
import openai
from datetime import datetime, timedelta

@dataclass
class SearchResult:
    """Resultado de busca estruturado"""
    endpoint_id: str
    source: str
    relevance_score: float
    category: str
    name: str
    summary: str
    keywords: List[str]
    confidence: float

class EvolutionAPIConstructor:
    """
    üß† Agente Constructor/Consultor da Evolution API

    Especialista em Evolution API com conhecimento de endpoints nativos e customizados.

    üöÄ NOVA ESTRAT√âGIA H√çBRIDA INTELIGENTE:
    1. Busca textual otimizada (sempre, r√°pida, sem custo)
    2. IA seletiva apenas para casos duvidosos (threshold configur√°vel: 75%)

    Resultado: 80-90% consultas resolvidas sem IA, mantendo alta qualidade.

    Suporte multi-provider:
    - Anthropic Claude (claude-3-5-sonnet, claude-sonnet-4)
    - OpenAI (o1-mini, o1-preview, gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo)
    """

    def __init__(self, config_path: str = "endpoints-and-hooks/config/ai_config.json"):
        self.config = self._load_config(config_path)
        self.ai_client = self._init_ai_client()
        self.consolidated_index = self._load_consolidated_index()
        self.cache = {}
        self.cache_timestamps = {}

        # üå∂Ô∏è PHASE 2: Contextual Enhancement - Configura√ß√µes
        self.context_files = {
            "filters": "endpoints-and-hooks/custom/filters.md",
            "webhooks": "endpoints-and-hooks/custom/dual-webhook-system.md"
        }

        # üéØ NOVO: Mapeamento Endpoint Nome ‚Üí Tag nos arquivos complementares
        self.endpoint_tag_mapping = {
            # Filters.md
            "Criar Inst√¢ncia": "CREATE_INSTANCE",
            "Consultar Filtros": "GET_FILTERS",
            "Atualizar Filtros": "UPDATE_FILTERS",
            "Consultar Filtros de √Åudio": "GET_AUDIO_FILTERS",
            "Atualizar Filtros de √Åudio": "UPDATE_AUDIO_FILTERS",
            "Estat√≠sticas de Filtros de √Åudio": "GET_AUDIO_STATS",
            "Resetar Estat√≠sticas de √Åudio": "RESET_AUDIO_STATS",
            "Estat√≠sticas da Fila": "GET_QUEUE_STATS",
            "Consultar Configura√ß√£o Global": "GET_WEBHOOK_CONFIG",
            "Criar/Atualizar Configura√ß√£o Global": "POST_WEBHOOK_CONFIG",

            # Dual-webhook-system.md
            "Sa√∫de dos Webhooks": "WEBHOOK_HEALTH",
            "M√©tricas dos Webhooks": "WEBHOOK_METRICS",
            "Consultar Logs": "WEBHOOK_LOGS",
            "Exportar Logs": "WEBHOOK_LOGS_EXPORT",
            "Listar Falhas": "WEBHOOK_FAILED",
            "Remover Falha Espec√≠fica": "DELETE_FAILED_WEBHOOK",
            "Limpar Todas as Falhas": "DELETE_ALL_FAILED",
            "Testar Webhook": "WEBHOOK_TEST",
            "Limpar Logs": "DELETE_LOGS"
        }

        # üéØ NOVO: Endpoints que devem ser enriquecidos com filters.md
        self.filter_endpoints = {
            "Criar Inst√¢ncia", "Consultar Filtros", "Atualizar Filtros",
            "Consultar Filtros de √Åudio", "Atualizar Filtros de √Åudio",
            "Estat√≠sticas de Filtros de √Åudio", "Resetar Estat√≠sticas de √Åudio",
            "Estat√≠sticas da Fila", "Consultar Configura√ß√£o Global",
            "Criar/Atualizar Configura√ß√£o Global"
        }

        # üéØ NOVO: Endpoints que devem ser enriquecidos com dual-webhook-system.md
        self.webhook_endpoints = {
            "Sa√∫de dos Webhooks", "M√©tricas dos Webhooks", "Consultar Logs",
            "Exportar Logs", "Listar Falhas", "Remover Falha Espec√≠fica",
            "Limpar Todas as Falhas", "Testar Webhook", "Limpar Logs"
        }

    def _load_config(self, config_path: str) -> Dict:
        """Carrega configura√ß√µes de AI e API keys"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"‚ùå Arquivo de configura√ß√£o n√£o encontrado: {config_path}")

    def _init_ai_client(self):
        """Inicializa cliente de IA baseado no provider configurado"""
        provider = self.config['current_provider']

        if provider == 'anthropic':
            return anthropic.Anthropic(
                api_key=self.config['anthropic']['api_key']
            )
        elif provider == 'openai':
            return openai.OpenAI(
                api_key=self.config['openai']['api_key']
            )
        else:
            raise ValueError(f"‚ùå Provider n√£o suportado: {provider}")

    def _load_consolidated_index(self) -> Dict:
        """Carrega √≠ndice consolidado (√∫nica vez)"""
        index_path = "endpoints-and-hooks/consolidated-map.json"
        try:
            with open(index_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"üìö √çndice carregado: {data['metadata']['total_entries']} endpoints/webhooks")
                return data
        except FileNotFoundError:
            raise FileNotFoundError(f"‚ùå √çndice consolidado n√£o encontrado: {index_path}")

    def _phase1_ai_probabilistic_ranking(self, user_query: str, all_endpoints: List[Dict]) -> List[SearchResult]:
        """
        ü§ñ FASE 1: Ranking Probabil√≠stico via IA (Uma √∫nica chamada)

        Nova estrat√©gia otimizada:
        1. Envia lista completa em formato tabular para IA
        2. IA retorna tabela com probabilidades
        3. Converte resultado em SearchResults
        4. Retorna TOP 3 candidatos

        Reduz de ~4-5 chamadas para 1 √∫nica chamada
        """

        print(f"ü§ñ Fase 1: Enviando {len(all_endpoints)} endpoints para ranking via IA (chamada √∫nica)...")

        # Preparar tabela para IA
        endpoints_table = self._prepare_endpoints_table(all_endpoints)

        # Faz uma √∫nica chamada para IA
        ai_probabilities = self._ai_single_call_ranking(user_query, endpoints_table)

        if not ai_probabilities:
            # Fallback para estrat√©gia textual se IA falhar
            print("‚ö†Ô∏è IA falhou, usando fallback textual...")
            return self._fallback_textual_ranking(user_query, all_endpoints)

        # Converte probabilidades IA em SearchResults
        scored_endpoints = []
        min_threshold = self.config.get('scoring', {}).get('minimum_threshold', 0.2)

        for prob_data in ai_probabilities:
            if prob_data['probability'] >= min_threshold:
                original_endpoint = all_endpoints[prob_data['index']]

                scored_endpoints.append(SearchResult(
                    endpoint_id=original_endpoint.get('id', f"endpoint_{prob_data['index']}"),
                    source=original_endpoint['source'],
                    relevance_score=prob_data['probability'],
                    category=original_endpoint['category'],
                    name=original_endpoint['name'],
                    summary=original_endpoint['summary'],
                    keywords=original_endpoint['keywords'],
                    confidence=prob_data.get('confidence', prob_data['probability'])
                ))

        # Ordena por probabilidade e retorna TOP 3
        sorted_results = sorted(scored_endpoints, key=lambda x: x.relevance_score, reverse=True)

        if self.config.get('debug_mode', False):
            print(f"üîç Fase 1: {len(sorted_results)} candidatos encontrados via IA")
            for i, result in enumerate(sorted_results[:5]):
                print(f"  {i+1}. {result.name} (Prob: {result.relevance_score:.3f})")

        return sorted_results[:3]  # TOP 3 para Fase 2

    def _hybrid_ranking_strategy(self, user_query: str, all_endpoints: List[Dict]) -> List[SearchResult]:
        """
        üß† NOVA ESTRAT√âGIA H√çBRIDA: Textual + IA Seletiva

        Fluxo inteligente:
        1. SEMPRE executa busca textual otimizada (r√°pida, sem custo)
        2. Se score >= threshold (75%): aceita resultado textual
        3. Se score < threshold: chama IA para resolver d√∫vida

        Resultado: 80-90% consultas resolvidas sem IA, mantendo alta qualidade
        """

        print(f"üß† Estrat√©gia H√≠brida: Textual primeiro, IA seletiva...")

        # FASE 1: Busca Textual Otimizada (sempre executada)
        print(f"üìä Fase 1: Executando busca textual...")
        textual_candidates = self._enhanced_textual_ranking(user_query, all_endpoints)

        if not textual_candidates:
            print("‚ùå Nenhum candidato textual encontrado")
            return []

        best_textual_score = textual_candidates[0].relevance_score
        confidence_threshold = self.config.get('hybrid_strategy', {}).get('textual_confidence_threshold', 0.75)

        print(f"üéØ Melhor score textual: {best_textual_score:.3f}")
        print(f"üìè Threshold de confian√ßa: {confidence_threshold}")

        # DECIS√ÉO INTELIGENTE: Aceita textual ou chama IA?
        if best_textual_score >= confidence_threshold:
            print(f"‚úÖ ALTA CONFIAN√áA: Score {best_textual_score:.3f} >= {confidence_threshold}")
            print(f"üöÄ Resultado textual aceito - SEM chamada IA")
            return textual_candidates

        else:
            print(f"‚ö†Ô∏è BAIXA CONFIAN√áA: Score {best_textual_score:.3f} < {confidence_threshold}")
            print(f"ü§ñ Chamando IA para resolver d√∫vida...")

            # FASE 2: IA apenas para casos duvidosos
            ai_candidates = self._phase1_ai_probabilistic_ranking(user_query, all_endpoints)

            if ai_candidates and len(ai_candidates) > 0:
                best_ai_score = ai_candidates[0].relevance_score
                print(f"ü§ñ IA retornou score: {best_ai_score:.3f}")

                # Compara IA vs Textual e escolhe o melhor
                if best_ai_score > best_textual_score * 1.1:  # IA deve ser 10% melhor
                    print(f"‚úÖ IA √© melhor: {best_ai_score:.3f} > {best_textual_score:.3f}")
                    return ai_candidates
                else:
                    print(f"‚úÖ Textual mantido: IA n√£o foi significativamente melhor")
                    return textual_candidates
            else:
                print(f"‚ö†Ô∏è IA falhou, mantendo resultado textual")
                return textual_candidates

    def _enhanced_textual_ranking(self, user_query: str, all_endpoints: List[Dict]) -> List[SearchResult]:
        """
        üìä Busca Textual Otimizada (vers√£o melhorada do fallback)

        Usa os keywords otimizados que j√° provaram funcionar perfeitamente
        """
        query_lower = user_query.lower()
        scored_endpoints = []
        min_threshold = self.config.get('scoring', {}).get('minimum_threshold', 0.2)

        # Pesos configur√°veis
        weights = self.config.get('scoring', {})
        name_weight = weights.get('name_weight', 0.4)
        summary_weight = weights.get('summary_weight', 0.35)
        keywords_weight = weights.get('keywords_weight', 0.25)

        for i, endpoint in enumerate(all_endpoints):
            # Textos para an√°lise
            name_text = endpoint.get('name', '').lower()
            summary_text = endpoint.get('summary', '').lower()
            keywords_text = ' '.join(endpoint.get('keywords', [])).lower()

            # Score por componente (como a vers√£o original, mas otimizada)
            name_score = self._calculate_text_similarity(query_lower, name_text)
            summary_score = self._calculate_text_similarity(query_lower, summary_text)
            keywords_score = self._calculate_text_similarity(query_lower, keywords_text)

            # Score ponderado final
            weighted_score = (
                name_score * name_weight +
                summary_score * summary_weight +
                keywords_score * keywords_weight
            )

            if weighted_score >= min_threshold:
                scored_endpoints.append(SearchResult(
                    endpoint_id=endpoint.get('id', f"endpoint_{i}"),
                    source=endpoint['source'],
                    relevance_score=weighted_score,
                    category=endpoint['category'],
                    name=endpoint['name'],
                    summary=endpoint['summary'],
                    keywords=endpoint['keywords'],
                    confidence=weighted_score
                ))

        # Ordena e retorna TOP 3
        sorted_results = sorted(scored_endpoints, key=lambda x: x.relevance_score, reverse=True)

        if self.config.get('debug_mode', False):
            print(f"üîç Busca textual: {len(sorted_results)} candidatos encontrados")
            for i, result in enumerate(sorted_results[:3]):
                print(f"  {i+1}. {result.name} (Score: {result.relevance_score:.3f})")

        return sorted_results[:3]

    def _prepare_endpoints_table(self, all_endpoints: List[Dict]) -> str:
        """
        Prepara tabela formatada para IA com ID | Nome | Keywords | Summary
        """
        lines = ["ID | Nome | Keywords | Summary"]
        lines.append("---|------|----------|--------")

        for i, endpoint in enumerate(all_endpoints):
            endpoint_id = endpoint.get('id', f"endpoint_{i}")
            name = endpoint['name'][:50]  # Limita tamanho
            keywords = ', '.join(endpoint['keywords'][:5])  # Limita keywords
            summary = endpoint['summary'][:100]  # Limita summary

            # Escapa pipes para n√£o quebrar a tabela
            name = name.replace('|', '\\|')
            keywords = keywords.replace('|', '\\|')
            summary = summary.replace('|', '\\|')

            lines.append(f"{endpoint_id} | {name} | {keywords} | {summary}")

        return '\n'.join(lines)

    def _ai_single_call_ranking(self, user_query: str, endpoints_table: str) -> List[Dict]:
        """
        ü§ñ Faz uma √∫nica chamada para IA para rankear todos os endpoints

        Retorna lista com probabilidades para cada endpoint
        """

        provider = self.config['current_provider']

        if provider == 'anthropic':
            return self._ai_single_call_anthropic(user_query, endpoints_table)
        elif provider == 'openai':
            return self._ai_single_call_openai(user_query, endpoints_table)
        else:
            print(f"‚ùå Provider n√£o suportado: {provider}")
            return []

    def _ai_single_call_anthropic(self, user_query: str, endpoints_table: str) -> List[Dict]:
        """
        Chamada √∫nica para Anthropic Claude
        """

        system_prompt = """
        Voc√™ √© um especialista em APIs REST e especificamente na Evolution API para WhatsApp.

        TAREFA: Analisar uma consulta do usu√°rio e calcular a probabilidade de cada endpoint da tabela corresponder √† inten√ß√£o da consulta.

        RETORNE um JSON com esta estrutura EXATA:
        {
            "rankings": [
                {
                    "id": "endpoint_id",
                    "index": 0,
                    "probability": 0.95,
                    "reasoning": "Breve explica√ß√£o do match"
                }
            ]
        }

        CRIT√âRIOS DE PROBABILIDADE:
        - 0.90-1.00: Match perfeito - endpoint faz exatamente o que o usu√°rio quer
        - 0.70-0.89: Match muito bom - endpoint resolve o problema com algumas adapta√ß√µes
        - 0.40-0.69: Match moderado - endpoint est√° relacionado mas n√£o √© ideal
        - 0.20-0.39: Match fraco - endpoint tem alguma rela√ß√£o distante
        - 0.00-0.19: Sem match - endpoint n√£o tem rela√ß√£o com a consulta

        IMPORTANTE:
        - Seja rigoroso: a maioria dos endpoints devem ter probabilidades baixas (< 0.3)
        - Apenas endpoints realmente relevantes devem ter probabilidades altas
        - Use seu conhecimento da Evolution API para validar se faz sentido
        - Considere sin√¥nimos e varia√ß√µes lingu√≠sticas (criar/adicionar, buscar/listar, etc.)
        - Analise a inten√ß√£o do usu√°rio (configurar vs consultar vs deletar)
        """

        user_prompt = f"""
        CONSULTA DO USU√ÅRIO: "{user_query}"

        TABELA DE ENDPOINTS:
        {endpoints_table}

        Analise cada endpoint da tabela e calcule a probabilidade dele corresponder √† consulta.
        Retorne o JSON com probabilidades para TODOS os endpoints listados.
        Use o √≠ndice da linha (come√ßando em 0) como "index".
        """

        try:
            response = self.ai_client.messages.create(
                model=self.config['anthropic']['model'],
                max_tokens=8000,  # Aumenta para tabela grande
                temperature=self.config['anthropic']['temperature_phase1'],
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )

            ai_response = response.content[0].text
            cleaned_response = self._clean_ai_response(ai_response)
            result = json.loads(cleaned_response)

            return result.get('rankings', [])

        except Exception as e:
            print(f"‚ùå Erro na an√°lise Anthropic: {e}")
            return []

    def _clean_ai_response(self, ai_response: str) -> str:
        """
        üßπ Limpa resposta da IA removendo blocos de c√≥digo markdown
        """
        # Remove blocos de c√≥digo markdown se existirem
        if '```json' in ai_response:
            # Extrai JSON entre ```json e ```
            start = ai_response.find('```json') + 7
            end = ai_response.find('```', start)
            if end != -1:
                return ai_response[start:end].strip()
        elif '```' in ai_response:
            # Extrai JSON entre ``` e ```
            start = ai_response.find('```') + 3
            end = ai_response.find('```', start)
            if end != -1:
                return ai_response[start:end].strip()

        # Se n√£o h√° blocos de c√≥digo, retorna original
        return ai_response.strip()

    def _ai_single_call_openai(self, user_query: str, endpoints_table: str) -> List[Dict]:
        """
        Chamada √∫nica para OpenAI
        """

        # Detecta se modelo suporta system messages
        model = self.config['openai']['model']
        supports_system = not model.startswith('o1-')  # o1 models n√£o suportam system

        system_prompt = """
        Voc√™ √© um especialista em APIs REST e especificamente na Evolution API para WhatsApp.

        TAREFA: Analisar uma consulta do usu√°rio e calcular a probabilidade de cada endpoint da tabela corresponder √† inten√ß√£o da consulta.

        RETORNE um JSON com esta estrutura EXATA:
        {
            "rankings": [
                {
                    "id": "endpoint_id",
                    "index": 0,
                    "probability": 0.95,
                    "reasoning": "Breve explica√ß√£o do match"
                }
            ]
        }

        CRIT√âRIOS DE PROBABILIDADE:
        - 0.90-1.00: Match perfeito - endpoint faz exatamente o que o usu√°rio quer
        - 0.70-0.89: Match muito bom - endpoint resolve o problema com algumas adapta√ß√µes
        - 0.40-0.69: Match moderado - endpoint est√° relacionado mas n√£o √© ideal
        - 0.20-0.39: Match fraco - endpoint tem alguma rela√ß√£o distante
        - 0.00-0.19: Sem match - endpoint n√£o tem rela√ß√£o com a consulta

        IMPORTANTE:
        - Seja rigoroso: a maioria dos endpoints devem ter probabilidades baixas (< 0.3)
        - Apenas endpoints realmente relevantes devem ter probabilidades altas
        - Use seu conhecimento da Evolution API para validar se faz sentido
        - Considere sin√¥nimos e varia√ß√µes lingu√≠sticas (criar/adicionar, buscar/listar, etc.)
        - Analise a inten√ß√£o do usu√°rio (configurar vs consultar vs deletar)
        """

        user_prompt = f"""
        CONSULTA DO USU√ÅRIO: "{user_query}"

        TABELA DE ENDPOINTS:
        {endpoints_table}

        Analise cada endpoint da tabela e calcule a probabilidade dele corresponder √† consulta.
        Retorne o JSON com probabilidades para TODOS os endpoints listados.
        Use o √≠ndice da linha (come√ßando em 0) como "index".
        """

        try:
            # Prepara mensagens baseado no suporte a system
            if supports_system:
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            else:
                # Para modelos o1, combina system e user em uma √∫nica mensagem
                combined_prompt = f"{system_prompt}\n\n{user_prompt}"
                messages = [
                    {"role": "user", "content": combined_prompt}
                ]

            response = self.ai_client.chat.completions.create(
                model=model,
                max_tokens=8000,
                temperature=self.config['openai']['temperature_phase1'],
                messages=messages
            )

            ai_response = response.choices[0].message.content

            # Limpa resposta de poss√≠veis blocos de c√≥digo markdown
            cleaned_response = self._clean_ai_response(ai_response)
            result = json.loads(cleaned_response)

            return result.get('rankings', [])

        except Exception as e:
            print(f"‚ùå Erro na an√°lise OpenAI: {e}")
            return []

    def _ai_calculate_probabilities(self, user_query: str, endpoints_for_ai: List[Dict]) -> List[Dict]:
        """
        ü§ñ IA calcula probabilidades de match para cada endpoint

        Envia lista completa para IA e recebe probabilidades estruturadas
        """

        # Divide endpoints em chunks para n√£o exceder limite de tokens
        chunk_size = 30  # Ajust√°vel baseado no tamanho m√©dio dos endpoints
        chunks = [endpoints_for_ai[i:i + chunk_size] for i in range(0, len(endpoints_for_ai), chunk_size)]

        all_probabilities = []

        for chunk_idx, chunk in enumerate(chunks):
            print(f"üîÑ Processando chunk {chunk_idx + 1}/{len(chunks)} ({len(chunk)} endpoints)...")

            chunk_probabilities = self._ai_process_endpoints_chunk(user_query, chunk)
            if chunk_probabilities:
                all_probabilities.extend(chunk_probabilities)

        # Ordena por probabilidade e retorna
        return sorted(all_probabilities, key=lambda x: x['probability'], reverse=True)

    def _ai_process_endpoints_chunk(self, user_query: str, endpoints_chunk: List[Dict]) -> List[Dict]:
        """
        ü§ñ Processa um chunk de endpoints via IA para calcular probabilidades
        """

        system_prompt = """
        Voc√™ √© um especialista em APIs REST e especificamente na Evolution API para WhatsApp.

        TAREFA: Analisar uma consulta do usu√°rio e calcular a probabilidade de cada endpoint corresponder √† inten√ß√£o da consulta.

        RETORNE um JSON com esta estrutura EXATA:
        {
            "rankings": [
                {
                    "index": 0,
                    "probability": 0.95,
                    "confidence": 0.90,
                    "reasoning": "Este endpoint corresponde exatamente porque..."
                },
                {
                    "index": 1,
                    "probability": 0.02,
                    "confidence": 0.85,
                    "reasoning": "N√£o corresponde porque..."
                }
            ]
        }

        CRIT√âRIOS DE PROBABILIDADE:
        - 0.90-1.00: Match perfeito - endpoint faz exatamente o que o usu√°rio quer
        - 0.70-0.89: Match muito bom - endpoint resolve o problema com algumas adapta√ß√µes
        - 0.40-0.69: Match moderado - endpoint est√° relacionado mas n√£o √© ideal
        - 0.20-0.39: Match fraco - endpoint tem alguma rela√ß√£o distante
        - 0.00-0.19: Sem match - endpoint n√£o tem rela√ß√£o com a consulta

        IMPORTANTE:
        - Seja rigoroso: a maioria dos endpoints devem ter probabilidades baixas (< 0.3)
        - Apenas endpoints realmente relevantes devem ter probabilidades altas
        - Use seu conhecimento da Evolution API para validar se faz sentido
        - Considere sin√¥nimos e varia√ß√µes lingu√≠sticas (criar/adicionar, buscar/listar, etc.)
        - Analise a inten√ß√£o do usu√°rio (configurar vs consultar vs deletar)
        """

        # Prepara lista compacta de endpoints para IA
        endpoints_summary = []
        for ep in endpoints_chunk:
            endpoints_summary.append(f"[{ep['index']}] {ep['name']} | {ep['category']} | {ep['summary']} | Keywords: {', '.join(ep['keywords'])}")

        user_prompt = f"""
        CONSULTA DO USU√ÅRIO: "{user_query}"

        ENDPOINTS PARA AN√ÅLISE:
        {chr(10).join(endpoints_summary)}

        Analise cada endpoint e calcule a probabilidade dele corresponder √† consulta.
        Retorne o JSON com probabilidades para TODOS os endpoints listados.
        """

        try:
            response = self.client.messages.create(
                model=self.config['anthropic']['model'],
                max_tokens=4000,  # Aumenta para lidar com mais endpoints
                temperature=self.config[self.config['current_provider']]['temperature_phase23'],  # Baixa para ser mais consistente
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )

            ai_response = response.content[0].text
            cleaned_response = self._clean_ai_response(ai_response)
            result = json.loads(cleaned_response)

            return result.get('rankings', [])

        except Exception as e:
            print(f"‚ùå Erro na an√°lise de probabilidades IA: {e}")
            return []

    def _fallback_textual_ranking(self, user_query: str, all_endpoints: List[Dict]) -> List[SearchResult]:
        """
        üìä Fallback: Ranking textual quando IA falha

        Vers√£o simplificada da estrat√©gia textual original
        """
        query_lower = user_query.lower()
        scored_endpoints = []
        min_threshold = self.config.get('scoring', {}).get('minimum_threshold', 0.2)

        for i, endpoint in enumerate(all_endpoints):
            # Textos para an√°lise
            name_text = endpoint.get('name', '').lower()
            summary_text = endpoint.get('summary', '').lower()
            keywords_text = ' '.join(endpoint.get('keywords', [])).lower()

            # Score simples baseado em matches
            name_score = self._calculate_text_similarity(query_lower, name_text)
            summary_score = self._calculate_text_similarity(query_lower, summary_text)
            keywords_score = self._calculate_text_similarity(query_lower, keywords_text)

            # Score combinado (pesos iguais para simplicidade)
            combined_score = (name_score + summary_score + keywords_score) / 3

            if combined_score >= min_threshold:
                scored_endpoints.append(SearchResult(
                    endpoint_id=endpoint.get('id', f"endpoint_{i}"),
                    source=endpoint['source'],
                    relevance_score=combined_score,
                    category=endpoint['category'],
                    name=endpoint['name'],
                    summary=endpoint['summary'],
                    keywords=endpoint['keywords'],
                    confidence=combined_score * 0.8  # Menor confian√ßa para fallback
                ))

        # Ordena e retorna TOP 3
        sorted_results = sorted(scored_endpoints, key=lambda x: x.relevance_score, reverse=True)
        return sorted_results[:3]

    def _normalize_text(self, text: str) -> str:
        """
        üîß NOVO: Normaliza texto removendo acentos para melhor matching
        """
        # Remove acentos e converte para min√∫sculas
        normalized = unicodedata.normalize('NFD', text.lower())
        return normalized.encode('ascii', 'ignore').decode('ascii')

    def _calculate_text_similarity(self, query: str, text: str) -> float:
        """
        üîß MELHORADO: Calcula similaridade com normaliza√ß√£o de acentos
        """
        if not text:
            return 0.0

        # Normaliza textos para melhor matching
        query_norm = self._normalize_text(query)
        text_norm = self._normalize_text(text)

        query_words = set(query_norm.split())
        text_words = set(text_norm.split())

        # 1. Exact match (peso alto)
        if query_norm in text_norm:
            return 1.0

        # 2. Partial match das palavras
        word_matches = len(query_words.intersection(text_words))
        word_score = word_matches / len(query_words) if query_words else 0

        # 3. Substring matches
        substring_score = 0
        for q_word in query_words:
            if len(q_word) > 3:  # S√≥ palavras significativas
                for t_word in text_words:
                    if q_word in t_word or t_word in q_word:
                        substring_score += 1
        substring_score = min(substring_score / len(query_words), 1.0) if query_words else 0

        # Score final combinado
        final_score = max(word_score, substring_score * 0.7)

        return final_score

    def _fallback_text_search(self, query: str, candidates: List[Dict]) -> List[SearchResult]:
        """Busca textual de fallback"""
        query_lower = query.lower()
        scored_results = []

        for candidate in candidates:
            score = 0.0
            text_to_search = f"{candidate['name']} {candidate['summary']} {' '.join(candidate['keywords'])}".lower()

            # Scoring simples por palavras-chave
            query_words = query_lower.split()
            for word in query_words:
                if word in text_to_search:
                    score += 1.0 / len(query_words)

            if score > 0:
                scored_results.append(SearchResult(
                    endpoint_id=candidate['id'],
                    source=candidate['source'],
                    relevance_score=score,
                    category=candidate['category'],
                    name=candidate['name'],
                    summary=candidate['summary'],
                    keywords=candidate['keywords'],
                    confidence=score * 0.7  # Menor confian√ßa para fallback
                ))

        return sorted(scored_results, key=lambda x: x.relevance_score, reverse=True)

    # üå∂Ô∏è PHASE 2: Contextual Enhancement Methods (NOVO SISTEMA)

    def _should_enrich_with_filters(self, endpoint_name: str) -> bool:
        """
        üéØ NOVA L√ìGICA: Busca exata por nome de endpoint (n√£o keywords)
        """
        return endpoint_name in self.filter_endpoints

    def _should_enrich_with_webhook(self, endpoint_name: str) -> bool:
        """
        üéØ NOVA L√ìGICA: Busca exata por nome de endpoint (n√£o keywords)
        """
        return endpoint_name in self.webhook_endpoints

    def _get_complement_files(self, endpoint_name: str) -> List[str]:
        """
        üéØ NOVA L√ìGICA: Determina quais arquivos de complemento devem ser consultados
        """
        files_to_enrich = []

        if self._should_enrich_with_filters(endpoint_name):
            files_to_enrich.append("filters")

        if self._should_enrich_with_webhook(endpoint_name):
            files_to_enrich.append("webhooks")

        return files_to_enrich

    def _extract_literal_complement_section(self, file_path: str, endpoint_tag: str) -> Optional[str]:
        """
        üìÑ NOVA FUN√á√ÉO: Extra√ß√£o literal de se√ß√£o espec√≠fica do arquivo complementar

        Busca por <!-- ENDPOINT:XXXX --> e extrai at√© pr√≥ximo delimitador:
        - <!-- ENDPOINT:
        - <!-- SECTION:
        - <!-- SUBSECTION:
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Busca in√≠cio da se√ß√£o
            start_marker = f"<!-- ENDPOINT:{endpoint_tag} -->"
            start_pos = content.find(start_marker)

            if start_pos == -1:
                print(f"‚ö†Ô∏è Tag {endpoint_tag} n√£o encontrada em {file_path}")
                return None

            # Busca fim da se√ß√£o (pr√≥ximo delimitador)
            end_markers = ["<!-- ENDPOINT:", "<!-- SECTION:", "<!-- SUBSECTION:"]
            end_pos = len(content)  # Default: at√© o fim do arquivo

            # Procura a partir do final do start_marker
            search_start = start_pos + len(start_marker)

            for marker in end_markers:
                marker_pos = content.find(marker, search_start)
                if marker_pos != -1 and marker_pos < end_pos:
                    end_pos = marker_pos

            # Extrai se√ß√£o literal
            section_content = content[start_pos:end_pos].strip()

            if self.config.get('debug_mode', False):
                print(f"‚úÖ Se√ß√£o {endpoint_tag} extra√≠da: {len(section_content)} chars")

            return section_content

        except Exception as e:
            print(f"‚ùå Erro ao extrair se√ß√£o {endpoint_tag} de {file_path}: {e}")
            return None

    def _extract_other_sections_from_complement(self, file_path: str) -> str:
        """
        üìö NOVA FUN√á√ÉO: Extrai outras se√ß√µes do arquivo complementar (n√£o de endpoints)

        Busca por se√ß√µes como:
        - <!-- SECTION:PRACTICAL_SCENARIOS -->
        - <!-- SECTION:ERROR_CODES -->
        - <!-- SECTION:CONFIGURATION -->
        etc.
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Se√ß√µes relevantes para contexto (n√£o de endpoints)
            relevant_sections = []

            # Busca todas as se√ß√µes que n√£o s√£o de endpoints
            import re
            section_pattern = r'<!-- SECTION:([^>]+) -->'
            sections = re.findall(section_pattern, content)

            for section_name in sections:
                if section_name not in ['API_ENDPOINTS', 'HEADER']:  # Skip endpoint sections
                    section_marker = f"<!-- SECTION:{section_name} -->"
                    section_start = content.find(section_marker)

                    if section_start != -1:
                        # Encontra pr√≥xima se√ß√£o ou fim do arquivo
                        next_section = content.find("<!-- SECTION:", section_start + len(section_marker))
                        if next_section == -1:
                            next_section = len(content)

                        section_content = content[section_start:next_section].strip()
                        relevant_sections.append(f"=== {section_name} ===\n{section_content}")

            return "\n\n".join(relevant_sections) if relevant_sections else ""

        except Exception as e:
            print(f"‚ùå Erro ao extrair outras se√ß√µes de {file_path}: {e}")
            return ""

    def _detect_context_needs(self, endpoint_info: Dict) -> List[str]:
        """
        üîç Detec√ß√£o Contextual P√≥s-Sele√ß√£o

        Baseado no ENDPOINT selecionado (n√£o na query), detecta quais arquivos
        de contexto devem ser carregados para enriquecimento.
        """
        context_needed = []

        # Textos para an√°lise contextual
        endpoint_text = f"{endpoint_info.get('name', '')} {endpoint_info.get('category', '')} {endpoint_info.get('summary', '')}".lower()

        # Detec√ß√£o de contexto de filtros
        if any(trigger in endpoint_text for trigger in self.filter_triggers):
            context_needed.append("filters")

        # Detec√ß√£o de contexto de webhooks
        if any(trigger in endpoint_text for trigger in self.webhook_triggers):
            context_needed.append("webhooks")

        return context_needed

    def _extract_contextual_information(self, context_type: str, endpoint_info: Dict) -> Optional[Dict]:
        """
        üìö Extra√ß√£o de Contexto Especializado

        Carrega arquivo de suporte e extrai se√ß√µes relevantes ao endpoint usando IA.
        """
        if context_type not in self.context_files:
            return None

        context_file = self.context_files[context_type]

        try:
            # Carrega arquivo de contexto completo
            with open(context_file, 'r', encoding='utf-8') as f:
                context_content = f.read()

            # IA extrai informa√ß√µes relevantes ao endpoint
            return self._ai_extract_relevant_context(context_content, context_type, endpoint_info)

        except Exception as e:
            print(f"‚ùå Erro ao carregar contexto {context_type}: {e}")
            return None

    def _ai_extract_relevant_context(self, context_content: str, context_type: str, endpoint_info: Dict) -> Dict:
        """
        ü§ñ IA extrai se√ß√µes relevantes do arquivo de contexto
        """

        system_prompt = f"""
        Voc√™ √© um especialista em documenta√ß√£o t√©cnica da Evolution API.

        TAREFA: Extrair informa√ß√µes relevantes de um arquivo de contexto especializado para enriquecer a documenta√ß√£o de um endpoint espec√≠fico.

        TIPO DE CONTEXTO: {context_type}

        RETORNE um JSON com esta estrutura:
        {{
            "source_file": "{context_type}.md",
            "practical_scenarios": [
                {{
                    "title": "Nome do Cen√°rio",
                    "description": "Descri√ß√£o do caso de uso",
                    "config": {{"key": "value"}},
                    "behavior": "Como o sistema se comporta"
                }}
            ],
            "important_notes": [
                "Informa√ß√£o cr√≠tica 1",
                "Limita√ß√£o importante 2"
            ],
            "troubleshooting": {{
                "common_errors": ["Erro comum 1", "Erro comum 2"],
                "solutions": ["Solu√ß√£o 1", "Solu√ß√£o 2"]
            }},
            "related_endpoints": [
                "/endpoint/relacionado1 - Descri√ß√£o",
                "/endpoint/relacionado2 - Descri√ß√£o"
            ]
        }}

        IMPORTANTE:
        - Extraia apenas informa√ß√µes RELEVANTES ao endpoint fornecido
        - Foque em cen√°rios pr√°ticos e dicas de uso
        - Inclua troubleshooting espec√≠fico se dispon√≠vel
        - Identifique endpoints relacionados mencionados no contexto
        - Se n√£o houver informa√ß√£o relevante, retorne campos vazios
        """

        user_prompt = f"""
        ENDPOINT ALVO:
        - Nome: {endpoint_info.get('name', '')}
        - Categoria: {endpoint_info.get('category', '')}
        - Resumo: {endpoint_info.get('summary', '')}

        CONTE√öDO DO ARQUIVO DE CONTEXTO:
        {context_content[:8000]}  # Limita para n√£o exceder tokens

        Extraia informa√ß√µes relevantes do contexto que ajudem a entender melhor este endpoint espec√≠fico.
        """

        try:
            provider = self.config['current_provider']

            if provider == 'anthropic':
                response = self.ai_client.messages.create(
                    model=self.config['anthropic']['model'],
                    max_tokens=2000,
                    temperature=self.config[self.config['current_provider']]['temperature_phase23'],
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                )
                ai_response = response.content[0].text

            elif provider == 'openai':
                model = self.config['openai']['model']
                supports_system = not model.startswith('o1-')

                if supports_system:
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                else:
                    combined_prompt = f"{system_prompt}\n\n{user_prompt}"
                    messages = [{"role": "user", "content": combined_prompt}]

                response = self.ai_client.chat.completions.create(
                    model=model,
                    max_tokens=2000,
                    temperature=self.config[self.config['current_provider']]['temperature_phase23'],
                    messages=messages
                )
                ai_response = response.choices[0].message.content

            else:
                raise ValueError(f"Provider n√£o suportado: {provider}")

            cleaned_response = self._clean_ai_response(ai_response)
            return json.loads(cleaned_response)

        except Exception as e:
            print(f"‚ùå Erro na extra√ß√£o de contexto IA: {e}")
            return {
                "source_file": f"{context_type}.md",
                "practical_scenarios": [],
                "important_notes": [],
                "troubleshooting": {"common_errors": [], "solutions": []},
                "related_endpoints": []
            }

    def _enrich_response_with_context(self, base_response: Dict, endpoint_name: str, user_query: str) -> Dict:
        """
        üå∂Ô∏è NOVA FASE 3: Enriquecimento Contextual com Extra√ß√£o Literal

        Nova estrat√©gia:
        1. Detecta arquivos complementares por nome exato do endpoint
        2. Extrai se√ß√µes literais dos arquivos
        3. IA gera observa√ß√µes contextuais
        """

        # Detecta quais arquivos de complemento devem ser consultados
        complement_files = self._get_complement_files(endpoint_name)

        if not complement_files:
            # Nenhum enriquecimento necess√°rio
            return base_response

        print(f"üîç Complementos detectados: {', '.join(complement_files)}")

        # Extrai conte√∫do literal dos complementos
        complement_content = {}
        other_sections_content = {}

        for file_type in complement_files:
            # Obter tag do endpoint para este arquivo
            endpoint_tag = self.endpoint_tag_mapping.get(endpoint_name)

            if not endpoint_tag:
                print(f"‚ö†Ô∏è Tag n√£o encontrada para endpoint: {endpoint_name}")
                continue

            file_path = self.context_files[file_type]

            # Extrai se√ß√£o espec√≠fica do endpoint
            literal_section = self._extract_literal_complement_section(file_path, endpoint_tag)
            if literal_section:
                if file_type == "filters":
                    complement_content["complemento-filter"] = literal_section
                elif file_type == "webhooks":
                    complement_content["complemento-webhook"] = literal_section

            # Extrai outras se√ß√µes relevantes
            other_sections = self._extract_other_sections_from_complement(file_path)
            if other_sections:
                other_sections_content[file_type] = other_sections

        # Adiciona conte√∫do literal ao response
        base_response.update(complement_content)

        # Gera observa√ß√µes contextuais via IA (se h√° complementos)
        if complement_content or other_sections_content:
            observations = self._generate_contextual_observations(
                user_query, endpoint_name, complement_content, other_sections_content
            )
            if observations:
                base_response["observacao"] = observations

            print(f"‚ú® Resposta enriquecida com {len(complement_content)} complementos")

        return base_response

    def _generate_contextual_observations(self, user_query: str, endpoint_name: str,
                                        complement_content: Dict, other_sections_content: Dict) -> Optional[str]:
        """
        ü§ñ NOVA FUN√á√ÉO: Gera observa√ß√µes contextuais usando IA

        Analisa o conte√∫do literal extra√≠do e outras se√ß√µes para gerar observa√ß√µes pr√°ticas.
        """
        if not complement_content and not other_sections_content:
            return None

        # Prepara conte√∫do para IA
        content_summary = []
        for key, content in complement_content.items():
            content_summary.append(f"=== {key.upper()} ===\n{content}")

        other_info_summary = []
        for file_type, content in other_sections_content.items():
            other_info_summary.append(f"=== OUTRAS SE√á√ïES DE {file_type.upper()} ===\n{content}")

        content_text = "\n\n".join(content_summary) if content_summary else ""
        other_info_text = "\n\n".join(other_info_summary) if other_info_summary else ""

        system_prompt = """
        Voc√™ √© um especialista em Evolution API.

        TAREFA: Analisar o endpoint selecionado e informa√ß√µes complementares para gerar observa√ß√µes pr√°ticas.

        OBJETIVO: Contextualizar a resposta para o usu√°rio com:
        1. Pontos de aten√ß√£o espec√≠ficos para este endpoint
        2. Cen√°rios pr√°ticos relacionados √† consulta original
        3. Troubleshooting comum
        4. Configura√ß√µes importantes
        5. Relacionamentos com outros endpoints

        IMPORTANTE:
        - Seja conciso e pr√°tico
        - Foque no que realmente ajuda o usu√°rio
        - Use informa√ß√µes dos arquivos complementares literais
        - Relate √† consulta original do usu√°rio
        - M√°ximo 200 palavras
        """

        user_prompt = f"""
        CONSULTA ORIGINAL DO USU√ÅRIO: "{user_query}"

        ENDPOINT ENCONTRADO: "{endpoint_name}"

        CONTE√öDO LITERAL EXTRA√çDO DOS COMPLEMENTOS:
        {content_text}

        OUTRAS SE√á√ïES DOS ARQUIVOS COMPLEMENTARES (N√ÉO DE ENDPOINTS):
        {other_info_text}

        Analise essas informa√ß√µes e gere observa√ß√µes pr√°ticas para contextualizar a resposta ao usu√°rio.
        """

        try:
            provider = self.config['current_provider']

            if provider == 'anthropic':
                response = self.ai_client.messages.create(
                    model=self.config['anthropic']['model'],
                    max_tokens=1000,
                    temperature=self.config[self.config['current_provider']]['temperature_phase23'],
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                )
                return response.content[0].text.strip()

            elif provider == 'openai':
                model = self.config['openai']['model']
                supports_system = not model.startswith('o1-')

                if supports_system:
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                else:
                    combined_prompt = f"{system_prompt}\n\n{user_prompt}"
                    messages = [{"role": "user", "content": combined_prompt}]

                response = self.ai_client.chat.completions.create(
                    model=model,
                    max_tokens=1000,
                    temperature=self.config[self.config['current_provider']]['temperature_phase23'],
                    messages=messages
                )
                return response.choices[0].message.content.strip()

            else:
                raise ValueError(f"Provider n√£o suportado: {provider}")

        except Exception as e:
            print(f"‚ùå Erro na gera√ß√£o de observa√ß√µes: {e}")
            return None

    def search_api(self, user_query: str) -> Dict:
        """
        üéØ FLUXO COMPLETO: Scoring + IA Validation + Contextual Enhancement

        Fluxo:
        1. FASE 1: Scoring ponderado textual (nome 40% + resumo 35% + keywords 25%)
        2. Seleciona TOP 2 candidatos
        3. FASE 2: IA valida o PRIMEIRO candidato com backup strategy
        4. üå∂Ô∏è FASE 3: Enriquecimento contextual especializado baseado no endpoint
        5. Retorna resultado estruturado + contexto enriquecido
        """

        print(f"üîç Buscando: '{user_query}'")

        # Cache check
        if self._is_cached(user_query):
            print("üíæ Resultado encontrado no cache")
            return self.cache[user_query]

        # NOVA ESTRAT√âGIA H√çBRIDA: Textual primeiro, IA apenas se necess√°rio
        all_endpoints = (self.consolidated_index.get('endpoints', []) +
                        self.consolidated_index.get('webhooks', []))

        top_candidates = self._hybrid_ranking_strategy(user_query, all_endpoints)

        if not top_candidates:
            return {"error": "Nenhum endpoint encontrado para a consulta", "query": user_query}

        print(f"üìä Fase 1: TOP 3 candidatos selecionados por IA")
        for i, candidate in enumerate(top_candidates[:3], 1):
            print(f"  {i}¬∫: {candidate.name} (Prob: {candidate.relevance_score:.3f})")

        # FASE 2: IA Validation com Strategy de M√∫ltiplos Candidatos
        first_candidate = top_candidates[0]
        second_candidate = top_candidates[1] if len(top_candidates) > 1 else None
        third_candidate = top_candidates[2] if len(top_candidates) > 2 else None

        print(f"ü§ñ Fase 2: Validando candidatos em ordem de probabilidade...")

        # Testa primeiro candidato (mais prov√°vel)
        first_result = self._extract_detailed_info_with_ai_validation(user_query, first_candidate)

        # Strategy: testa m√∫ltiplos candidatos se primeiro n√£o for convincente
        candidate_results = []
        if first_result:
            candidate_results.append({
                'candidate': first_candidate,
                'result': first_result,
                'ai_score': first_result.get('final_score', 0),
                'probability': first_candidate.relevance_score
            })

        # Testa segundo candidato se primeira probabilidade IA n√£o for convincente
        if second_candidate and first_result:
            first_ai_score = first_result.get('final_score', 0)
            first_probability = first_candidate.relevance_score

            # Se score IA for muito menor que probabilidade IA, testa outros
            if first_ai_score < first_probability * 0.8:  # 20% de degrada√ß√£o
                print(f"‚ö†Ô∏è  Score final ({first_ai_score:.3f}) < Probabilidade IA ({first_probability:.3f})")
                print(f"üîÑ Testando segundo candidato...")

                second_result = self._extract_detailed_info_with_ai_validation(user_query, second_candidate)
                if second_result:
                    candidate_results.append({
                        'candidate': second_candidate,
                        'result': second_result,
                        'ai_score': second_result.get('final_score', 0),
                        'probability': second_candidate.relevance_score
                    })

        # Testa terceiro se necess√°rio
        if third_candidate and len(candidate_results) == 2:
            best_score = max(r['ai_score'] for r in candidate_results)
            if best_score < 0.7:  # Se nenhum dos dois primeiros for convincente
                print(f"üîÑ Testando terceiro candidato...")
                third_result = self._extract_detailed_info_with_ai_validation(user_query, third_candidate)
                if third_result:
                    candidate_results.append({
                        'candidate': third_candidate,
                        'result': third_result,
                        'ai_score': third_result.get('final_score', 0),
                        'probability': third_candidate.relevance_score
                    })

        # Seleciona o melhor resultado baseado no score final da IA
        if candidate_results:
            best_result = max(candidate_results, key=lambda x: x['ai_score'])
            final_result = best_result['result']
            selected_candidate = best_result['candidate']

            print(f"‚úÖ Melhor candidato: {selected_candidate.name} (Score final: {best_result['ai_score']:.3f})")
        else:
            final_result = None
            selected_candidate = first_candidate

        if not final_result:
            return {"error": "Nenhum resultado detalhado encontrado", "query": user_query}

        # üå∂Ô∏è FASE 3: Enriquecimento Contextual (NOVA IMPLEMENTA√á√ÉO)
        print(f"üå∂Ô∏è Fase 3: Analisando necessidade de enriquecimento contextual...")

        # Aplica enriquecimento contextual com nova l√≥gica
        enriched_result = self._enrich_response_with_context(final_result, selected_candidate.name, user_query)

        # Cache do resultado enriquecido
        self._cache_result(user_query, enriched_result)

        return enriched_result

    def _initial_keyword_filter(self, query: str, endpoints: List[Dict]) -> List[Dict]:
        """Filtragem inicial por keywords e texto"""
        query_words = set(query.lower().split())
        candidates = []

        for endpoint in endpoints:
            # Cria texto para busca
            search_text = f"{endpoint['name']} {endpoint['summary']} {' '.join(endpoint['keywords'])}".lower()

            # Verifica se alguma palavra da query est√° presente
            if any(word in search_text for word in query_words):
                candidates.append(endpoint)

            # Busca por matches parciais tamb√©m
            elif any(word in search_text for word in query_words if len(word) > 3):
                candidates.append(endpoint)

        return candidates[:20]  # Limita candidatos

    def _extract_detailed_info_with_ai_validation(self, user_query: str, candidate: SearchResult) -> Optional[Dict]:
        """
        üìù Extra√ß√£o detalhada das informa√ß√µes do endpoint
        Fase 2: Confirma√ß√£o do match + estrutura√ß√£o completa
        """

        try:
            # Carrega map espec√≠fico
            map_path = f"endpoints-and-hooks/{candidate.source}/map.json"
            with open(map_path, 'r', encoding='utf-8') as f:
                map_data = json.load(f)

            # Encontra localiza√ß√£o no map
            location_info = self._find_endpoint_in_map(candidate.name, map_data)

            if not location_info:
                print(f"‚ùå Localiza√ß√£o n√£o encontrada no map para {candidate.name}")
                return None

            # Extrai conte√∫do espec√≠fico do description.md
            description_path = f"endpoints-and-hooks/{candidate.source}/description.md"

            # Determina linha final baseado na disponibilidade de response
            start_line = location_info['request']['startLine']
            if 'response' in location_info:
                end_line = location_info['response']['endLine']
            else:
                end_line = location_info['request']['endLine']

            detailed_content = self._extract_content_by_location(
                description_path,
                start_line,
                end_line
            )

            if not detailed_content:
                print(f"‚ùå Conte√∫do n√£o extra√≠do para {candidate.name}")
                return None

            # IA processa e estrutura a informa√ß√£o final
            return self._ai_structure_final_response(user_query, candidate, location_info, detailed_content)

        except Exception as e:
            print(f"‚ùå Erro na extra√ß√£o detalhada: {e}")
            return None

    def _find_endpoint_in_map(self, endpoint_name: str, map_data: Dict) -> Optional[Dict]:
        """Encontra endpoint espec√≠fico no map.json usando o nome exato"""

        for category, data in map_data.items():
            if isinstance(data, dict) and 'endpoints' in data:
                for endpoint in data['endpoints']:
                    # Busca por nome exato
                    if endpoint.get('name', '') == endpoint_name:
                        return endpoint.get('location', {})

        return None

    def _extract_content_by_location(self, file_path: str, start_line: int, end_line: int) -> str:
        """Extra√ß√£o cir√∫rgica de conte√∫do por linhas espec√≠ficas"""

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # Ajusta √≠ndices (1-based para 0-based)
            start_idx = max(0, start_line - 1)
            end_idx = min(len(lines), end_line)

            # Adiciona contexto extra se necess√°rio
            context_start = max(0, start_idx - 2)
            context_end = min(len(lines), end_idx + 2)

            return ''.join(lines[context_start:context_end])

        except Exception as e:
            print(f"‚ùå Erro ao extrair conte√∫do: {e}")
            return ""

    def _ai_structure_final_response(self, user_query: str, candidate: SearchResult,
                                   location_info: Dict, content: str) -> Dict:
        """
        üìÑ NOVA ESTRUTURA: Retorna informa√ß√µes b√°sicas + documenta√ß√£o literal

        Nova estrat√©gia sem IA excessiva:
        1. Dados b√°sicos do endpoint
        2. Documenta√ß√£o literal do description.md
        3. Score de match
        """

        # Informa√ß√µes b√°sicas do endpoint extra√≠das diretamente
        basic_info = {
            "endpoint": {
                "name": candidate.name,
                "category": candidate.category,
                "source": candidate.source,
                "summary": candidate.summary
            },
            "documentacao": content.strip(),  # CONTE√öDO LITERAL do description.md
            "final_score": candidate.relevance_score,
            "confidence": candidate.confidence,
            "match_reasoning": f"Endpoint '{candidate.name}' selecionado por correspond√™ncia com '{user_query}'"
        }

        return basic_info

    def _is_cached(self, query: str) -> bool:
        """Verifica se resultado est√° em cache v√°lido"""
        if not self.config.get('cache_enabled', True):
            return False

        if query in self.cache:
            cache_time = self.cache_timestamps.get(query)
            if cache_time:
                ttl = timedelta(seconds=self.config.get('cache_ttl_seconds', 3600))
                return datetime.now() - cache_time < ttl

        return False

    def _cache_result(self, query: str, result: Dict):
        """Armazena resultado no cache"""
        if self.config.get('cache_enabled', True):
            self.cache[query] = result
            self.cache_timestamps[query] = datetime.now()


def optimized_constructor(user_query: str, config_path: str = "endpoints-and-hooks/config/ai_config.json") -> Dict:
    """
    üéØ FUN√á√ÉO PRINCIPAL DO CONSTRUCTOR

    Ponto de entrada para busca e constru√ß√£o de endpoints da Evolution API.

    Args:
        user_query: Consulta do usu√°rio (ex: "como criar inst√¢ncia", "webhook de monitoramento")
        config_path: Caminho para arquivo de configura√ß√£o

    Returns:
        Dict: JSON estruturado com informa√ß√µes completas do endpoint
    """

    print("üöÄ Evolution API Constructor iniciado")
    print(f"üìù Query: '{user_query}'")

    try:
        # Inicializa o agente
        agent = EvolutionAPIConstructor(config_path)

        # Executa busca h√≠brida
        result = agent.search_api(user_query)

        print("‚úÖ Busca conclu√≠da com sucesso")
        return result

    except Exception as e:
        error_result = {
            "error": f"Erro no constructor: {str(e)}",
            "query": user_query,
            "timestamp": datetime.now().isoformat()
        }
        print(f"‚ùå {error_result['error']}")
        return error_result


# üß™ Exemplo de uso
if __name__ == "__main__":
    # Teste b√°sico
    test_queries = [
        "como criar uma inst√¢ncia",
        "webhook de monitoramento",
        "filtros de √°udio por dura√ß√£o",
        "estat√≠sticas da fila global"
    ]

    for query in test_queries:
        print(f"\n{'='*50}")
        result = optimized_constructor(query)
        print(f"Resultado para '{query}':")
        print(json.dumps(result, indent=2, ensure_ascii=False))